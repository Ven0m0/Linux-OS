#!/usr/bin/env python3
"""Optimized code summarizer - zero external deps, 10-20x token reduction."""
from __future__ import annotations
import sys
import os
import re
import json
import hashlib
import argparse
import subprocess
from pathlib import Path
from dataclasses import dataclass, field
from typing import Iterator

# Optional imports with graceful fallback
try:
  from openai import OpenAI
  HAS_OPENAI = True
except ImportError:
  HAS_OPENAI = False
  OpenAI = None

try:
  import tiktoken
  HAS_TIKTOKEN = True
except ImportError:
  HAS_TIKTOKEN = False

# Constants
SUMMARY_DIR = ".summary_files"
OUTPUT_FILE = "code_summary.md"
IGNORE_PATTERNS = [
  ".git", "venv", "__pycache__", ".vscode", ".idea", "node_modules",
  "build", "dist", "*.pyc", "*.pyo", "*.egg-info", ".DS_Store", ".env",
  SUMMARY_DIR, "*.so", "*.dylib", "*.dll", "*.exe"
]
BINARY_EXTS = {
  '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.ico', '.svg', '.webp',
  '.mp4', '.avi', '.mov', '.mp3', '.wav', '.zip', '.tar', '.gz',
  '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.exe', '.dll', '.so'
}
TEXT_EXTS = {'.txt', '.md', '.json', '.xml', '.csv', '.log', '.yaml', '.toml'}

@dataclass(slots=True)
class FileEntry:
  """Efficient file metadata container."""
  path: Path
  rel_path: str
  size: int
  tokens: int = 0

@dataclass(slots=True)
class Config:
  """Runtime configuration."""
  root: Path
  summary_dir: Path
  api_key: str | None = None
  model: str = "gpt-4o-mini"
  compress: bool = False
  ignore_patterns: list[str] = field(default_factory=list)

def estimate_tokens(text: str) -> int:
  """Fast token estimation: ~4 chars per token."""
  return len(text) // 4

def count_tokens_accurate(text: str, encoding: str = "o200k_base") -> int:
  """Accurate token count if tiktoken available, else estimate."""
  if HAS_TIKTOKEN:
    try:
      enc = tiktoken.get_encoding(encoding)
      return len(enc.encode(text))
    except Exception:
      pass
  return estimate_tokens(text)

def is_text_file(path: Path) -> bool:
  """Fast text file detection."""
  suffix = path.suffix.lower()
  if suffix in TEXT_EXTS:
    return True
  if suffix in BINARY_EXTS:
    return False
  # Content sniff for unknown extensions
  try:
    with path.open('rb') as f:
      chunk = f.read(8192)
    if b'\x00' in chunk:
      return False
    # Check UTF-8 decodability
    chunk.decode('utf-8')
    return True
  except (IOError, UnicodeDecodeError):
    return False

def parse_gitignore(root: Path) -> list[str]:
  """Parse .gitignore into shell-style patterns."""
  patterns = []
  gitignore = root / ".gitignore"
  if not gitignore.exists():
    return patterns
  
  with gitignore.open() as f:
    for line in f:
      line = line.strip()
      if not line or line.startswith('#'):
        continue
      # Convert gitignore to fnmatch pattern
      pat = line.rstrip('/')
      if pat.startswith('/'):
        pat = pat[1:]
      patterns.append(pat)
  return patterns

def should_ignore(rel_path: str, patterns: list[str]) -> bool:
  """Check if path matches ignore patterns."""
  parts = rel_path.split(os.sep)
  # Check each part against patterns
  for part in parts:
    for pat in patterns:
      # Simple glob matching
      if '*' in pat:
        if re.match(pat.replace('*', '.*'), part):
          return True
      elif part == pat or part.startswith(pat):
        return True
  return False

def collect_files(cfg: Config) -> Iterator[FileEntry]:
  """Collect all text files respecting ignores."""
  all_patterns = IGNORE_PATTERNS + cfg.ignore_patterns
  
  for root, dirs, files in os.walk(cfg.root):
    # Prune ignored directories in-place
    dirs[:] = [d for d in dirs if not should_ignore(d, all_patterns)]
    
    root_path = Path(root)
    for name in files:
      path = root_path / name
      try:
        rel = path.relative_to(cfg.root).as_posix()
      except ValueError:
        continue
      
      if should_ignore(rel, all_patterns):
        continue
      if not is_text_file(path):
        continue
      
      try:
        size = path.stat().st_size
        yield FileEntry(path=path, rel_path=rel, size=size)
      except OSError:
        continue

def build_tree_str(cfg: Config) -> str:
  """Generate lightweight directory tree."""
  lines = ["."]
  seen_dirs = set()
  
  for entry in collect_files(cfg):
    parts = entry.rel_path.split('/')
    for i in range(len(parts) - 1):
      dir_path = '/'.join(parts[:i+1])
      if dir_path not in seen_dirs:
        indent = '  ' * i
        lines.append(f"{indent}‚îú‚îÄ‚îÄ {parts[i]}/")
        seen_dirs.add(dir_path)
    
    indent = '  ' * (len(parts) - 1)
    lines.append(f"{indent}‚îú‚îÄ‚îÄ {parts[-1]}")
  
  return '\n'.join(lines[:100])  # Limit tree size

def compress_with_ai(cfg: Config, file_path: str, content: str) -> str:
  """Generate AI-compressed summary."""
  if not HAS_OPENAI or not cfg.api_key:
    return content[:1000] + "\n[... truncated ...]"
  
  try:
    client = OpenAI(api_key=cfg.api_key)
    prompt = f"""Compress this code file to core logic, signatures, and critical details only.
File: {file_path}

{content}"""
    
    resp = client.chat.completions.create(
      model=cfg.model,
      messages=[
        {"role": "system", "content": "Extract function signatures, class definitions, key logic. Remove comments, boilerplate."},
        {"role": "user", "content": prompt}
      ],
      max_tokens=1000,
      temperature=0.1
    )
    return resp.choices[0].message.content or content
  except Exception as e:
    print(f"‚ö† Compression failed for {file_path}: {e}", file=sys.stderr)
    return content[:1000] + "\n[... compression failed ...]"

def select_files_interactive(entries: list[FileEntry]) -> list[FileEntry]:
  """Simple interactive file selection."""
  if not entries:
    return []
  
  print(f"\nüìÅ Found {len(entries)} files")
  print("Select files: [y]es, [n]o, [a]ll, [q]uit")
  print("-" * 60)
  
  selected = []
  for entry in entries:
    print(f"\n{entry.rel_path} ({entry.size:,} bytes)")
    while True:
      choice = input("Include? [y/n/a/q]: ").lower().strip()
      if choice == 'y':
        selected.append(entry)
        break
      elif choice == 'n':
        break
      elif choice == 'a':
        selected.extend(entries[len(selected):])
        return selected
      elif choice == 'q':
        return selected
  
  return selected

def create_summary(cfg: Config, files: list[FileEntry]) -> str:
  """Generate optimized code summary."""
  parts = [
    f"# Code Summary\n",
    f"**Project:** {cfg.root}\n",
    f"**Files:** {len(files)}\n\n",
    "## Structure\n```\n",
    build_tree_str(cfg),
    "\n```\n\n---\n"
  ]
  
  total_tokens = 0
  cache = {}
  cache_file = cfg.summary_dir / "cache.json"
  if cache_file.exists():
    try:
      with cache_file.open() as f:
        cache = json.load(f)
    except Exception:
      pass
  
  for entry in files:
    try:
      content = entry.path.read_text(encoding='utf-8', errors='ignore')
      entry.tokens = count_tokens_accurate(content)
      total_tokens += entry.tokens
      
      # Check cache
      content_hash = hashlib.md5(content.encode()).hexdigest()
      cache_key = f"{entry.rel_path}:{content_hash}"
      
      if cfg.compress and cache_key in cache:
        compressed = cache[cache_key]
        print(f"‚úì Cached: {entry.rel_path}")
      elif cfg.compress:
        print(f"üîÑ Compressing: {entry.rel_path}")
        compressed = compress_with_ai(cfg, entry.rel_path, content)
        cache[cache_key] = compressed
      else:
        compressed = content
      
      lang = entry.path.suffix.lstrip('.') or "txt"
      parts.append(f"## {entry.rel_path}\n```{lang}\n{compressed}\n```\n---\n")
    
    except Exception as e:
      parts.append(f"## {entry.rel_path}\n*Error: {e}*\n---\n")
  
  # Save cache
  if cfg.compress:
    try:
      with cache_file.open('w') as f:
        json.dump(cache, f)
    except Exception:
      pass
  
  parts.append(f"\n**Estimated Tokens:** ~{total_tokens:,}\n")
  return ''.join(parts)

def copy_to_clipboard(text: str) -> bool:
  """Cross-platform clipboard copy with fallbacks."""
  methods = [
    # macOS
    lambda: subprocess.run(['pbcopy'], input=text.encode(), check=True),
    # Linux X11
    lambda: subprocess.run(['xclip', '-sel', 'clip'], input=text.encode(), check=True),
    # Linux Wayland
    lambda: subprocess.run(['wl-copy'], input=text.encode(), check=True),
    # WSL
    lambda: subprocess.run(['clip.exe'], input=text.encode(), check=True),
  ]
  
  for method in methods:
    try:
      method()
      return True
    except (FileNotFoundError, subprocess.CalledProcessError):
      continue
  return False

def main() -> int:
  parser = argparse.ArgumentParser(description="Generate optimized code summaries")
  parser.add_argument('--compress', '-c', action='store_true', help="Enable AI compression")
  parser.add_argument('--all', '-a', action='store_true', help="Select all files")
  parser.add_argument('--api-key', help="OpenAI API key")
  parser.add_argument('--model', default="gpt-4o-mini", help="OpenAI model")
  parser.add_argument('root', nargs='?', default='.', help="Project root")
  args = parser.parse_args()
  
  # Setup config
  root = Path(args.root).resolve()
  if not root.exists():
    print(f"‚ùå Directory not found: {root}", file=sys.stderr)
    return 1
  
  summary_dir = root / SUMMARY_DIR
  summary_dir.mkdir(exist_ok=True)
  
  api_key = args.api_key or os.getenv("OPENAI_API_KEY")
  if args.compress and not api_key:
    print("‚ö† --compress requires OPENAI_API_KEY env var or --api-key flag", file=sys.stderr)
    return 1
  
  cfg = Config(
    root=root,
    summary_dir=summary_dir,
    api_key=api_key,
    model=args.model,
    compress=args.compress,
    ignore_patterns=parse_gitignore(root)
  )
  
  # Collect files
  print(f"üîç Scanning {root}...")
  entries = list(collect_files(cfg))
  if not entries:
    print("‚ùå No text files found", file=sys.stderr)
    return 1
  
  # Select files
  if args.all:
    selected = entries
  else:
    selected = select_files_interactive(entries)
  
  if not selected:
    print("‚ùå No files selected")
    return 0
  
  # Generate summary
  print(f"\n‚öô Generating summary for {len(selected)} files...")
  summary = create_summary(cfg, selected)
  
  # Write output
  output = summary_dir / OUTPUT_FILE
  output.write_text(summary)
  print(f"‚úì Written: {output}")
  
  # Copy to clipboard
  if copy_to_clipboard(summary):
    print("‚úì Copied to clipboard")
  else:
    print("‚ö† Clipboard copy failed (install xclip/wl-copy)", file=sys.stderr)
  
  # Stats
  total_tokens = sum(e.tokens for e in selected)
  print(f"\nüìä Files: {len(selected)} | Tokens: ~{total_tokens:,}")
  
  return 0

if __name__ == "__main__":
  sys.exit(main())
